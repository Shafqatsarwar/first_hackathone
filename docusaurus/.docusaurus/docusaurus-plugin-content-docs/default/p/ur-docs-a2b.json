{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/ur/docs/intro","label":"Physical AI Textbook Landing","docId":"intro","unlisted":false},{"type":"link","href":"/ur/docs/book-overview","label":"Book Overview","docId":"book-overview","unlisted":false},{"type":"link","href":"/ur/docs/module-01/physical-ai-nervous-system","label":"01 Physical AI Nervous System","docId":"module-01/physical-ai-nervous-system","unlisted":false},{"type":"link","href":"/ur/docs/module-01/ros2-dds-philosophy","label":"02 ROS 2 & DDS Philosophy","docId":"module-01/ros2-dds-philosophy","unlisted":false},{"type":"link","href":"/ur/docs/module-01/nodes-cognitive-units","label":"03 Nodes as Cognitive Units","docId":"module-01/nodes-cognitive-units","unlisted":false},{"type":"link","href":"/ur/docs/module-01/topics-services-actions","label":"04 Topics, Services, Actions","docId":"module-01/topics-services-actions","unlisted":false},{"type":"link","href":"/ur/docs/module-01/python-agents-rclpy","label":"06 Python Agents (rclpy)","docId":"module-01/python-agents-rclpy","unlisted":false},{"type":"link","href":"/ur/docs/module-01/llm-to-control","label":"07 LLMs & Control","docId":"module-01/llm-to-control","unlisted":false},{"type":"link","href":"/ur/docs/module-01/parameters-runtime","label":"08 Parameters & Runtime","docId":"module-01/parameters-runtime","unlisted":false},{"type":"link","href":"/ur/docs/module-01/urdf-humanoid-body","label":"09 URDF & Body","docId":"module-01/urdf-humanoid-body","unlisted":false},{"type":"link","href":"/ur/docs/module-01/actuators-feedback","label":"10 Actuators & Feedback","docId":"module-01/actuators-feedback","unlisted":false},{"type":"link","href":"/ur/docs/module-01/launch-composition","label":"11 Launch & Composition","docId":"module-01/launch-composition","unlisted":false},{"type":"link","href":"/ur/docs/module-02/digital-twins","label":"01 Digital Twins","docId":"module-02/digital-twins","unlisted":false},{"type":"link","href":"/ur/docs/module-02/physics-simulation","label":"02 Physics Simulation","docId":"module-02/physics-simulation","unlisted":false},{"type":"link","href":"/ur/docs/module-02/gazebo-ros2","label":"03 Gazebo + ROS 2","docId":"module-02/gazebo-ros2","unlisted":false},{"type":"link","href":"/ur/docs/module-02/gravity-collisions","label":"04 Gravity & Collisions","docId":"module-02/gravity-collisions","unlisted":false},{"type":"link","href":"/ur/docs/module-02/humanoid-locomotion","label":"05 Humanoid Locomotion","docId":"module-02/humanoid-locomotion","unlisted":false},{"type":"link","href":"/ur/docs/module-02/sensor-modeling","label":"06 Sensor Modeling","docId":"module-02/sensor-modeling","unlisted":false},{"type":"link","href":"/ur/docs/module-02/noise-latency","label":"07 Noise & Latency","docId":"module-02/noise-latency","unlisted":false},{"type":"link","href":"/ur/docs/module-02/unity-interaction","label":"08 Unity Interaction","docId":"module-02/unity-interaction","unlisted":false},{"type":"link","href":"/ur/docs/module-02/photorealism","label":"09 Photorealism","docId":"module-02/photorealism","unlisted":false},{"type":"link","href":"/ur/docs/module-02/domain-randomization","label":"10 Domain Randomization","docId":"module-02/domain-randomization","unlisted":false},{"type":"link","href":"/ur/docs/module-02/transfer-strategies","label":"11 Simulation-to-Real","docId":"module-02/transfer-strategies","unlisted":false},{"type":"link","href":"/ur/docs/module-02/module-summary","label":"Module 02 Summary","docId":"module-02/module-summary","unlisted":false},{"type":"link","href":"/ur/docs/module-03/accelerated-computing","label":"01 Accelerated Computing","docId":"module-03/accelerated-computing","unlisted":false},{"type":"link","href":"/ur/docs/module-03/isaac-sim-architecture","label":"02 Isaac Sim Architecture","docId":"module-03/isaac-sim-architecture","unlisted":false},{"type":"link","href":"/ur/docs/module-03/gpu-physics-perception","label":"03 GPU Physics & Perception","docId":"module-03/gpu-physics-perception","unlisted":false},{"type":"link","href":"/ur/docs/module-03/synthetic-data","label":"04 Synthetic Data","docId":"module-03/synthetic-data","unlisted":false},{"type":"link","href":"/ur/docs/module-03/training-vision-models","label":"05 Vision Models","docId":"module-03/training-vision-models","unlisted":false},{"type":"link","href":"/ur/docs/module-03/visual-slam","label":"06 Visual SLAM","docId":"module-03/visual-slam","unlisted":false},{"type":"link","href":"/ur/docs/module-03/isaac-ros-hw","label":"07 Isaac ROS","docId":"module-03/isaac-ros-hw","unlisted":false},{"type":"link","href":"/ur/docs/module-03/localization-mapping","label":"08 Localization & Mapping","docId":"module-03/localization-mapping","unlisted":false},{"type":"link","href":"/ur/docs/module-03/nav2","label":"09 Nav2 Navigation","docId":"module-03/nav2","unlisted":false},{"type":"link","href":"/ur/docs/module-03/path-planning","label":"10 Path Planning","docId":"module-03/path-planning","unlisted":false},{"type":"link","href":"/ur/docs/module-03/autonomy-stack","label":"11 Autonomy Stack","docId":"module-03/autonomy-stack","unlisted":false},{"type":"link","href":"/ur/docs/module-03/module-summary","label":"Module 03 Summary","docId":"module-03/module-summary","unlisted":false},{"type":"link","href":"/ur/docs/module-04/vla-intro","label":"01 VLA Overview","docId":"module-04/vla-intro","unlisted":false},{"type":"link","href":"/ur/docs/module-04/llms-in-vla","label":"02 LLMs","docId":"module-04/llms-in-vla","unlisted":false},{"type":"link","href":"/ur/docs/module-04/vision-perception","label":"03 Vision Perception","docId":"module-04/vision-perception","unlisted":false},{"type":"link","href":"/ur/docs/module-04/multimodal-fusion","label":"04 Multimodal Fusion","docId":"module-04/multimodal-fusion","unlisted":false},{"type":"link","href":"/ur/docs/module-04/speech-and-voice","label":"05 Speech & Voice","docId":"module-04/speech-and-voice","unlisted":false},{"type":"link","href":"/ur/docs/module-04/action-planning","label":"06 Action Planning","docId":"module-04/action-planning","unlisted":false},{"type":"link","href":"/ur/docs/module-04/dialog-management","label":"07 Dialog Management","docId":"module-04/dialog-management","unlisted":false},{"type":"link","href":"/ur/docs/module-04/feedback-loop","label":"08 Feedback Loops","docId":"module-04/feedback-loop","unlisted":false},{"type":"link","href":"/ur/docs/module-04/safety","label":"09 Safety","docId":"module-04/safety","unlisted":false},{"type":"link","href":"/ur/docs/module-04/personalization","label":"10 Personalization","docId":"module-04/personalization","unlisted":false},{"type":"link","href":"/ur/docs/module-04/module-summary","label":"Module 04 Summary","docId":"module-04/module-summary","unlisted":false}]},"docs":{"book-overview":{"id":"book-overview","title":"Book Overview","description":"This overview explains how the textbook maps to the RAG chatbot modules and your learning workflow.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Physical AI Textbook Landing","description":"Welcome to the Physical AI Textbook, your guide to embodied intelligence and humanoid robotics.","sidebar":"tutorialSidebar"},"module-01/actuators-feedback":{"id":"module-01/actuators-feedback","title":"10. Actuators, Sensors, and Feedback Loops","description":"This chapter explains how sensors, actuators, and closed-loop feedback keep the humanoid's nervous system stable.","sidebar":"tutorialSidebar"},"module-01/launch-composition":{"id":"module-01/launch-composition","title":"11. Launch and Composition","description":"ROS 2 launch files and composition allow you to bring sets of nodes online deterministically so the humanoid nervous system boots with valid configurations.","sidebar":"tutorialSidebar"},"module-01/llm-to-control":{"id":"module-01/llm-to-control","title":"07. Bridging LLM Reasoning with Deterministic Control","description":"LLMs generate symbolic plans while deterministic controllers execute motor commands safely; the architecture must keep the two systems separated but coordinated.","sidebar":"tutorialSidebar"},"module-01/nodes-cognitive-units":{"id":"module-01/nodes-cognitive-units","title":"03. Nodes as Cognitive Units","description":"Each ROS 2 node functions as a cognitive microservice. This chapter covers best practices for dividing responsibilities, sharing parameters, and instrumenting nodes for telemetry.","sidebar":"tutorialSidebar"},"module-01/parameters-runtime":{"id":"module-01/parameters-runtime","title":"08. Parameters and Runtime Adaptation","description":"ROS parameters allow runtime tuning without redeployment, enabling personalization and adaptive behavior.","sidebar":"tutorialSidebar"},"module-01/physical-ai-nervous-system":{"id":"module-01/physical-ai-nervous-system","title":"01. Physical AI Nervous System Overview","description":"This chapter introduces the concept of a robotic nervous system built on ROS 2. Physical AI teams raw sensing, planning, and actuation by wiring sensors through ROS nodes so the humanoid platform can sense and react in real time.","sidebar":"tutorialSidebar"},"module-01/python-agents-rclpy":{"id":"module-01/python-agents-rclpy","title":"06. Python AI Agents with rclpy","description":"Python accelerates AI development. The rclpy client library lets Python agents subscribe to topics, call services, and interact with actions while keeping ROS 2 semantics intact.","sidebar":"tutorialSidebar"},"module-01/ros2-dds-philosophy":{"id":"module-01/ros2-dds-philosophy","title":"02. ROS 2 and DDS Philosophy","description":"ROS 2 relies on the DDS (Data Distribution Service) ecosystem to make pub-sub reliable, secure, and real-time capable. In this document, we review the quality of service settings, discovery processes, and node lifecycle that keep the nervous system healthy.","sidebar":"tutorialSidebar"},"module-01/topics-services-actions":{"id":"module-01/topics-services-actions","title":"04. Topics, Services, and Actions","description":"This chapter enumerates the communication primitives that ROS 2 exposes: topics for broadcast telemetry, services for synchronous control, and actions for cancellable behaviors.","sidebar":"tutorialSidebar"},"module-01/urdf-humanoid-body":{"id":"module-01/urdf-humanoid-body","title":"09. URDF and the Humanoid Body","description":"URDF defines links, joints, mass, inertia, and collision geometry, grounding AI in the robot's physical reality.","sidebar":"tutorialSidebar"},"module-02/digital-twins":{"id":"module-02/digital-twins","title":"01. Digital Twins in Embodied Intelligence","description":"A digital twin is a virtual replica of a humanoid robot, including sensors, actuators, dynamics, and control logic. It enables safe experimentation, scalable AI training, and rapid iteration without risking hardware.","sidebar":"tutorialSidebar"},"module-02/domain-randomization":{"id":"module-02/domain-randomization","title":"10. Domain Randomization Techniques","description":"Domain randomization adds variability (textures, lighting, positions) so models generalize beyond the training simulator.","sidebar":"tutorialSidebar"},"module-02/gazebo-ros2":{"id":"module-02/gazebo-ros2","title":"03. Gazebo Architecture and ROS 2 Integration","description":"Gazebo provides physics engine integration, sensor simulation, and real-time bridges to ROS 2 nodes to run multi-node systems end to end.","sidebar":"tutorialSidebar"},"module-02/gravity-collisions":{"id":"module-02/gravity-collisions","title":"04. Gravity, Collisions, and Contact Dynamics","description":"Collision detection and response plus gravity modeling are essential for humanoid locomotion and manipulation. Without them balance and impact handling fail.","sidebar":"tutorialSidebar"},"module-02/humanoid-locomotion":{"id":"module-02/humanoid-locomotion","title":"05. Humanoid Locomotion Simulation","description":"Humanoid locomotion needs center-of-mass control, precise foot placement, and torque-aware joints. Simulation lets you test gait strategies safely.","sidebar":"tutorialSidebar"},"module-02/module-summary":{"id":"module-02/module-summary","title":"12. Module 02 — Final Summary","description":"Digital twins allow humanoid robots to learn, plan, and interact safely in virtual environments.","sidebar":"tutorialSidebar"},"module-02/noise-latency":{"id":"module-02/noise-latency","title":"07. Noise, Latency, and Failure Injection","description":"Injecting noise, delays, and failures ensures AI models can handle adverse conditions and sensor imperfections.","sidebar":"tutorialSidebar"},"module-02/photorealism":{"id":"module-02/photorealism","title":"09. Photorealism and Synthetic Vision Data","description":"Synthetic datasets provide perfect annotations—segmentation masks, depth maps, and object labels—for perception model development.","sidebar":"tutorialSidebar"},"module-02/physics-simulation":{"id":"module-02/physics-simulation","title":"02. Physics Simulation Fundamentals","description":"Accurate physics simulation models gravity, collisions, contact dynamics, and joint constraints so control strategies behave reliably before hitting hardware.","sidebar":"tutorialSidebar"},"module-02/sensor-modeling":{"id":"module-02/sensor-modeling","title":"06. Sensor Modeling: Cameras, LiDAR, IMU","description":"Robotics sensors include RGB/depth cameras, LiDAR, and IMUs. Simulation adds realistic noise and latency to bridge the reality gap.","sidebar":"tutorialSidebar"},"module-02/transfer-strategies":{"id":"module-02/transfer-strategies","title":"11. Simulation-to-Real Transfer Strategies","description":"Deploy AI models from simulation to real humanoids with progressive testing, fine-tuning, and safety validation.","sidebar":"tutorialSidebar"},"module-02/unity-interaction":{"id":"module-02/unity-interaction","title":"08. Unity for Human–Robot Interaction","description":"Unity provides photorealistic rendering, interaction scenarios, and synthetic vision datasets for humanoid perception testing.","sidebar":"tutorialSidebar"},"module-03/accelerated-computing":{"id":"module-03/accelerated-computing","title":"01. Accelerated Computing in Physical AI","description":"GPUs accelerate physics, sensor rendering, and deep learning inference so humanoid robots can perceive and plan fast enough to stay upright and responsive.","sidebar":"tutorialSidebar"},"module-03/autonomy-stack":{"id":"module-03/autonomy-stack","title":"11. End-to-End Autonomy Stack","description":"Isaac Sim enables end-to-end testing of sensors, SLAM, planning, and control so the AI brain can be validated before deployment.","sidebar":"tutorialSidebar"},"module-03/gpu-physics-perception":{"id":"module-03/gpu-physics-perception","title":"03. GPU-Accelerated Physics and Perception","description":"Physics and vision computations benefit from CUDA and tensor cores for dynamics, collision, and sensor simulation workloads.","sidebar":"tutorialSidebar"},"module-03/isaac-ros-hw":{"id":"module-03/isaac-ros-hw","title":"07. Isaac ROS and Hardware Acceleration","description":"Isaac ROS exposes GPU-accelerated vision processing, mapping, and efficient ROS 2 communication with real hardware.","sidebar":"tutorialSidebar"},"module-03/isaac-sim-architecture":{"id":"module-03/isaac-sim-architecture","title":"02. NVIDIA Isaac Sim Architecture","description":"Isaac Sim provides photorealistic rendering, GPU-accelerated physics, and tight ROS 2 integration so AI brains can be built and validated on high-fidelity simulations.","sidebar":"tutorialSidebar"},"module-03/localization-mapping":{"id":"module-03/localization-mapping","title":"08. Localization and Mapping in Dynamic Environments","description":"Dynamic scenes require frequent map updates, sensor fusion, and adaptive planning to maintain accurate localization.","sidebar":"tutorialSidebar"},"module-03/module-summary":{"id":"module-03/module-summary","title":"Module 03 — Final Summary","description":"Isaac Sim provides a GPU-powered brain for humanoids. Synthetic data, Visual SLAM, Nav2, and physics-aware planning let AI-brain deployments stay safe, scalable, and ready for the real world.","sidebar":"tutorialSidebar"},"module-03/nav2":{"id":"module-03/nav2","title":"09. Navigation with Nav2 for Humanoids","description":"Nav2 handles costmaps, path planning, obstacle avoidance, and footstep planning tailored to bipedal robots.","sidebar":"tutorialSidebar"},"module-03/path-planning":{"id":"module-03/path-planning","title":"10. Path Planning Under Physical Constraints","description":"Humanoid robots require physics-aware planning to respect joint torque, balance, and collision avoidance.","sidebar":"tutorialSidebar"},"module-03/synthetic-data":{"id":"module-03/synthetic-data","title":"04. Synthetic Data Generation Pipelines","description":"Synthetic datasets deliver perfect annotations—diverse environments, labels, and scenarios—for robust perception training.","sidebar":"tutorialSidebar"},"module-03/training-vision-models":{"id":"module-03/training-vision-models","title":"05. Training Vision Models for Robots","description":"Isaac Sim generates photorealistic images, depth maps, and masks that enable supervised learning for detection and segmentation.","sidebar":"tutorialSidebar"},"module-03/visual-slam":{"id":"module-03/visual-slam","title":"06. Visual SLAM Fundamentals","description":"SLAM combines sensor data and odometry to estimate the robot's position, and Isaac ROS accelerates this with GPU support.","sidebar":"tutorialSidebar"},"module-04/action-planning":{"id":"module-04/action-planning","title":"06. Action Planning from Language","description":"Translate language instructions into deterministic sub-goals and real-time controllers.","sidebar":"tutorialSidebar"},"module-04/dialog-management":{"id":"module-04/dialog-management","title":"07. Dialog Management in Embodied AI","description":"Maintain multi-turn conversations while grounding context in sensor data and recent actions.","sidebar":"tutorialSidebar"},"module-04/feedback-loop":{"id":"module-04/feedback-loop","title":"08. Feedback Loops and Adaptation","description":"Continuous feedback from sensors and users lets VLA systems adapt behavior over time.","sidebar":"tutorialSidebar"},"module-04/llms-in-vla":{"id":"module-04/llms-in-vla","title":"02. Large Language Models for Robotics","description":"LLMs provide flexible instruction understanding, enabling natural queries, dialog management, and plan generation for robots.","sidebar":"tutorialSidebar"},"module-04/module-summary":{"id":"module-04/module-summary","title":"Module 04 — Final Summary","description":"Vision-Language-Action weaves speech, vision, and language reasoning into safe, adaptive robotic behaviors. Multimodal fusion, planning, and feedback loops deliver embodied intelligence with natural interaction.","sidebar":"tutorialSidebar"},"module-04/multimodal-fusion":{"id":"module-04/multimodal-fusion","title":"04. Multimodal Fusion","description":"Fusing vision and language embeddings allows the robot to ground instructions into perceived objects.","sidebar":"tutorialSidebar"},"module-04/personalization":{"id":"module-04/personalization","title":"10. Personalization and Context","description":"Tailor VLA responses based on user background, role, preferences, and the robot's current mission.","sidebar":"tutorialSidebar"},"module-04/safety":{"id":"module-04/safety","title":"09. Safety and Alignment","description":"Safety reviews ensure instructions do not violate robot or human integrity, especially when chaining complex commands.","sidebar":"tutorialSidebar"},"module-04/speech-and-voice":{"id":"module-04/speech-and-voice","title":"05. Speech and Voice Perception","description":"Speech recognition translates spoken commands into textual intent for the language-action pipeline.","sidebar":"tutorialSidebar"},"module-04/vision-perception":{"id":"module-04/vision-perception","title":"03. Vision Perception for VLA","description":"Vision feeds the language-action loop with object awareness, scene understanding, and affordances.","sidebar":"tutorialSidebar"},"module-04/vla-intro":{"id":"module-04/vla-intro","title":"01. VLA Overview","description":"Vision-Language-Action ties visual perception, natural language understanding, and actionable robot behavior to create intuitive humanoid interaction.","sidebar":"tutorialSidebar"}}}}