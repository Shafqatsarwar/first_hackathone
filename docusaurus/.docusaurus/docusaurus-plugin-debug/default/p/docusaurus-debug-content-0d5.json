{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"D:\\Panaverse\\project\\first_hackathone\\docusaurus\\sidebars.js","contentPath":"D:\\Panaverse\\project\\first_hackathone\\docusaurus\\docs","docs":[{"id":"book-overview","title":"Book Overview","description":"This overview explains how the textbook maps to the RAG chatbot modules and your learning workflow.","source":"@site/docs/book-overview.md","sourceDirName":".","slug":"/book-overview","permalink":"/docs/book-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/book-overview.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Book Overview"},"sidebar":"tutorialSidebar","previous":{"title":"Physical AI Textbook Landing","permalink":"/docs/intro"},"next":{"title":"01 Physical AI Nervous System","permalink":"/docs/module-01/physical-ai-nervous-system"}},{"id":"intro","title":"Physical AI Textbook Landing","description":"Welcome to the Physical AI Textbook, your guide to embodied intelligence and humanoid robotics.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Physical AI Textbook Landing"},"sidebar":"tutorialSidebar","next":{"title":"Book Overview","permalink":"/docs/book-overview"}},{"id":"module-01/actuators-feedback","title":"10. Actuators, Sensors, and Feedback Loops","description":"This chapter explains how sensors, actuators, and closed-loop feedback keep the humanoid's nervous system stable.","source":"@site/docs/module-01/10-actuators-feedback.md","sourceDirName":"module-01","slug":"/module-01/actuators-feedback","permalink":"/docs/module-01/actuators-feedback","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/10-actuators-feedback.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"10. Actuators, Sensors, and Feedback Loops","sidebar_label":"10 Actuators & Feedback"},"sidebar":"tutorialSidebar","previous":{"title":"09 URDF & Body","permalink":"/docs/module-01/urdf-humanoid-body"},"next":{"title":"11 Launch & Composition","permalink":"/docs/module-01/launch-composition"}},{"id":"module-01/launch-composition","title":"11. Launch and Composition","description":"ROS 2 launch files and composition allow you to bring sets of nodes online deterministically so the humanoid nervous system boots with valid configurations.","source":"@site/docs/module-01/11-launch-composition.md","sourceDirName":"module-01","slug":"/module-01/launch-composition","permalink":"/docs/module-01/launch-composition","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/11-launch-composition.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"11. Launch and Composition","sidebar_label":"11 Launch & Composition"},"sidebar":"tutorialSidebar","previous":{"title":"10 Actuators & Feedback","permalink":"/docs/module-01/actuators-feedback"},"next":{"title":"01 Digital Twins","permalink":"/docs/module-02/digital-twins"}},{"id":"module-01/llm-to-control","title":"07. Bridging LLM Reasoning with Deterministic Control","description":"LLMs generate symbolic plans while deterministic controllers execute motor commands safely; the architecture must keep the two systems separated but coordinated.","source":"@site/docs/module-01/07-llm-to-control.md","sourceDirName":"module-01","slug":"/module-01/llm-to-control","permalink":"/docs/module-01/llm-to-control","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/07-llm-to-control.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"07. Bridging LLM Reasoning with Deterministic Control","sidebar_label":"07 LLMs & Control"},"sidebar":"tutorialSidebar","previous":{"title":"06 Python Agents (rclpy)","permalink":"/docs/module-01/python-agents-rclpy"},"next":{"title":"08 Parameters & Runtime","permalink":"/docs/module-01/parameters-runtime"}},{"id":"module-01/nodes-cognitive-units","title":"03. Nodes as Cognitive Units","description":"Each ROS 2 node functions as a cognitive microservice. This chapter covers best practices for dividing responsibilities, sharing parameters, and instrumenting nodes for telemetry.","source":"@site/docs/module-01/03-nodes-cognitive-units.md","sourceDirName":"module-01","slug":"/module-01/nodes-cognitive-units","permalink":"/docs/module-01/nodes-cognitive-units","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/03-nodes-cognitive-units.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"03. Nodes as Cognitive Units","sidebar_label":"03 Nodes as Cognitive Units"},"sidebar":"tutorialSidebar","previous":{"title":"02 ROS 2 & DDS Philosophy","permalink":"/docs/module-01/ros2-dds-philosophy"},"next":{"title":"04 Topics, Services, Actions","permalink":"/docs/module-01/topics-services-actions"}},{"id":"module-01/parameters-runtime","title":"08. Parameters and Runtime Adaptation","description":"ROS parameters allow runtime tuning without redeployment, enabling personalization and adaptive behavior.","source":"@site/docs/module-01/08-parameters-runtime.md","sourceDirName":"module-01","slug":"/module-01/parameters-runtime","permalink":"/docs/module-01/parameters-runtime","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/08-parameters-runtime.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"08. Parameters and Runtime Adaptation","sidebar_label":"08 Parameters & Runtime"},"sidebar":"tutorialSidebar","previous":{"title":"07 LLMs & Control","permalink":"/docs/module-01/llm-to-control"},"next":{"title":"09 URDF & Body","permalink":"/docs/module-01/urdf-humanoid-body"}},{"id":"module-01/physical-ai-nervous-system","title":"01. Physical AI Nervous System Overview","description":"This chapter introduces the concept of a robotic nervous system built on ROS 2. Physical AI teams raw sensing, planning, and actuation by wiring sensors through ROS nodes so the humanoid platform can sense and react in real time.","source":"@site/docs/module-01/01-physical-ai-nervous-system.md","sourceDirName":"module-01","slug":"/module-01/physical-ai-nervous-system","permalink":"/docs/module-01/physical-ai-nervous-system","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/01-physical-ai-nervous-system.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"01. Physical AI Nervous System Overview","sidebar_label":"01 Physical AI Nervous System"},"sidebar":"tutorialSidebar","previous":{"title":"Book Overview","permalink":"/docs/book-overview"},"next":{"title":"02 ROS 2 & DDS Philosophy","permalink":"/docs/module-01/ros2-dds-philosophy"}},{"id":"module-01/python-agents-rclpy","title":"06. Python AI Agents with rclpy","description":"Python accelerates AI development. The rclpy client library lets Python agents subscribe to topics, call services, and interact with actions while keeping ROS 2 semantics intact.","source":"@site/docs/module-01/06-python-agents-rclpy.md","sourceDirName":"module-01","slug":"/module-01/python-agents-rclpy","permalink":"/docs/module-01/python-agents-rclpy","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/06-python-agents-rclpy.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"06. Python AI Agents with rclpy","sidebar_label":"06 Python Agents (rclpy)"},"sidebar":"tutorialSidebar","previous":{"title":"04 Topics, Services, Actions","permalink":"/docs/module-01/topics-services-actions"},"next":{"title":"07 LLMs & Control","permalink":"/docs/module-01/llm-to-control"}},{"id":"module-01/ros2-dds-philosophy","title":"02. ROS 2 and DDS Philosophy","description":"ROS 2 relies on the DDS (Data Distribution Service) ecosystem to make pub-sub reliable, secure, and real-time capable. In this document, we review the quality of service settings, discovery processes, and node lifecycle that keep the nervous system healthy.","source":"@site/docs/module-01/02-ros2-dds-philosophy.md","sourceDirName":"module-01","slug":"/module-01/ros2-dds-philosophy","permalink":"/docs/module-01/ros2-dds-philosophy","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/02-ros2-dds-philosophy.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"02. ROS 2 and DDS Philosophy","sidebar_label":"02 ROS 2 & DDS Philosophy"},"sidebar":"tutorialSidebar","previous":{"title":"01 Physical AI Nervous System","permalink":"/docs/module-01/physical-ai-nervous-system"},"next":{"title":"03 Nodes as Cognitive Units","permalink":"/docs/module-01/nodes-cognitive-units"}},{"id":"module-01/topics-services-actions","title":"04. Topics, Services, and Actions","description":"This chapter enumerates the communication primitives that ROS 2 exposes: topics for broadcast telemetry, services for synchronous control, and actions for cancellable behaviors.","source":"@site/docs/module-01/04-topics-services-actions.md","sourceDirName":"module-01","slug":"/module-01/topics-services-actions","permalink":"/docs/module-01/topics-services-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/04-topics-services-actions.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"04. Topics, Services, and Actions","sidebar_label":"04 Topics, Services, Actions"},"sidebar":"tutorialSidebar","previous":{"title":"03 Nodes as Cognitive Units","permalink":"/docs/module-01/nodes-cognitive-units"},"next":{"title":"06 Python Agents (rclpy)","permalink":"/docs/module-01/python-agents-rclpy"}},{"id":"module-01/urdf-humanoid-body","title":"09. URDF and the Humanoid Body","description":"URDF defines links, joints, mass, inertia, and collision geometry, grounding AI in the robot's physical reality.","source":"@site/docs/module-01/09-urdf-humanoid-body.md","sourceDirName":"module-01","slug":"/module-01/urdf-humanoid-body","permalink":"/docs/module-01/urdf-humanoid-body","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-01/09-urdf-humanoid-body.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"09. URDF and the Humanoid Body","sidebar_label":"09 URDF & Body"},"sidebar":"tutorialSidebar","previous":{"title":"08 Parameters & Runtime","permalink":"/docs/module-01/parameters-runtime"},"next":{"title":"10 Actuators & Feedback","permalink":"/docs/module-01/actuators-feedback"}},{"id":"module-02/digital-twins","title":"01. Digital Twins in Embodied Intelligence","description":"A digital twin is a virtual replica of a humanoid robot, including sensors, actuators, dynamics, and control logic. It enables safe experimentation, scalable AI training, and rapid iteration without risking hardware.","source":"@site/docs/module-02/01-digital-twins.md","sourceDirName":"module-02","slug":"/module-02/digital-twins","permalink":"/docs/module-02/digital-twins","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/01-digital-twins.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"01. Digital Twins in Embodied Intelligence","sidebar_label":"01 Digital Twins"},"sidebar":"tutorialSidebar","previous":{"title":"11 Launch & Composition","permalink":"/docs/module-01/launch-composition"},"next":{"title":"02 Physics Simulation","permalink":"/docs/module-02/physics-simulation"}},{"id":"module-02/domain-randomization","title":"10. Domain Randomization Techniques","description":"Domain randomization adds variability (textures, lighting, positions) so models generalize beyond the training simulator.","source":"@site/docs/module-02/10-domain-randomization.md","sourceDirName":"module-02","slug":"/module-02/domain-randomization","permalink":"/docs/module-02/domain-randomization","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/10-domain-randomization.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"10. Domain Randomization Techniques","sidebar_label":"10 Domain Randomization"},"sidebar":"tutorialSidebar","previous":{"title":"09 Photorealism","permalink":"/docs/module-02/photorealism"},"next":{"title":"11 Simulation-to-Real","permalink":"/docs/module-02/transfer-strategies"}},{"id":"module-02/gazebo-ros2","title":"03. Gazebo Architecture and ROS 2 Integration","description":"Gazebo provides physics engine integration, sensor simulation, and real-time bridges to ROS 2 nodes to run multi-node systems end to end.","source":"@site/docs/module-02/03-gazebo-ros2.md","sourceDirName":"module-02","slug":"/module-02/gazebo-ros2","permalink":"/docs/module-02/gazebo-ros2","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/03-gazebo-ros2.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"03. Gazebo Architecture and ROS 2 Integration","sidebar_label":"03 Gazebo + ROS 2"},"sidebar":"tutorialSidebar","previous":{"title":"02 Physics Simulation","permalink":"/docs/module-02/physics-simulation"},"next":{"title":"04 Gravity & Collisions","permalink":"/docs/module-02/gravity-collisions"}},{"id":"module-02/gravity-collisions","title":"04. Gravity, Collisions, and Contact Dynamics","description":"Collision detection and response plus gravity modeling are essential for humanoid locomotion and manipulation. Without them balance and impact handling fail.","source":"@site/docs/module-02/04-gravity-collisions.md","sourceDirName":"module-02","slug":"/module-02/gravity-collisions","permalink":"/docs/module-02/gravity-collisions","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/04-gravity-collisions.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"04. Gravity, Collisions, and Contact Dynamics","sidebar_label":"04 Gravity & Collisions"},"sidebar":"tutorialSidebar","previous":{"title":"03 Gazebo + ROS 2","permalink":"/docs/module-02/gazebo-ros2"},"next":{"title":"05 Humanoid Locomotion","permalink":"/docs/module-02/humanoid-locomotion"}},{"id":"module-02/humanoid-locomotion","title":"05. Humanoid Locomotion Simulation","description":"Humanoid locomotion needs center-of-mass control, precise foot placement, and torque-aware joints. Simulation lets you test gait strategies safely.","source":"@site/docs/module-02/05-humanoid-locomotion.md","sourceDirName":"module-02","slug":"/module-02/humanoid-locomotion","permalink":"/docs/module-02/humanoid-locomotion","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/05-humanoid-locomotion.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"05. Humanoid Locomotion Simulation","sidebar_label":"05 Humanoid Locomotion"},"sidebar":"tutorialSidebar","previous":{"title":"04 Gravity & Collisions","permalink":"/docs/module-02/gravity-collisions"},"next":{"title":"06 Sensor Modeling","permalink":"/docs/module-02/sensor-modeling"}},{"id":"module-02/module-summary","title":"12. Module 02 — Final Summary","description":"Digital twins allow humanoid robots to learn, plan, and interact safely in virtual environments.","source":"@site/docs/module-02/12-module-summary.md","sourceDirName":"module-02","slug":"/module-02/module-summary","permalink":"/docs/module-02/module-summary","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/12-module-summary.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"12. Module 02 — Final Summary","sidebar_label":"Module 02 Summary"},"sidebar":"tutorialSidebar","previous":{"title":"11 Simulation-to-Real","permalink":"/docs/module-02/transfer-strategies"},"next":{"title":"01 Accelerated Computing","permalink":"/docs/module-03/accelerated-computing"}},{"id":"module-02/noise-latency","title":"07. Noise, Latency, and Failure Injection","description":"Injecting noise, delays, and failures ensures AI models can handle adverse conditions and sensor imperfections.","source":"@site/docs/module-02/07-noise-latency.md","sourceDirName":"module-02","slug":"/module-02/noise-latency","permalink":"/docs/module-02/noise-latency","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/07-noise-latency.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"07. Noise, Latency, and Failure Injection","sidebar_label":"07 Noise & Latency"},"sidebar":"tutorialSidebar","previous":{"title":"06 Sensor Modeling","permalink":"/docs/module-02/sensor-modeling"},"next":{"title":"08 Unity Interaction","permalink":"/docs/module-02/unity-interaction"}},{"id":"module-02/photorealism","title":"09. Photorealism and Synthetic Vision Data","description":"Synthetic datasets provide perfect annotations—segmentation masks, depth maps, and object labels—for perception model development.","source":"@site/docs/module-02/09-photorealism.md","sourceDirName":"module-02","slug":"/module-02/photorealism","permalink":"/docs/module-02/photorealism","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/09-photorealism.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"09. Photorealism and Synthetic Vision Data","sidebar_label":"09 Photorealism"},"sidebar":"tutorialSidebar","previous":{"title":"08 Unity Interaction","permalink":"/docs/module-02/unity-interaction"},"next":{"title":"10 Domain Randomization","permalink":"/docs/module-02/domain-randomization"}},{"id":"module-02/physics-simulation","title":"02. Physics Simulation Fundamentals","description":"Accurate physics simulation models gravity, collisions, contact dynamics, and joint constraints so control strategies behave reliably before hitting hardware.","source":"@site/docs/module-02/02-physics-simulation.md","sourceDirName":"module-02","slug":"/module-02/physics-simulation","permalink":"/docs/module-02/physics-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/02-physics-simulation.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"02. Physics Simulation Fundamentals","sidebar_label":"02 Physics Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"01 Digital Twins","permalink":"/docs/module-02/digital-twins"},"next":{"title":"03 Gazebo + ROS 2","permalink":"/docs/module-02/gazebo-ros2"}},{"id":"module-02/sensor-modeling","title":"06. Sensor Modeling: Cameras, LiDAR, IMU","description":"Robotics sensors include RGB/depth cameras, LiDAR, and IMUs. Simulation adds realistic noise and latency to bridge the reality gap.","source":"@site/docs/module-02/06-sensor-modeling.md","sourceDirName":"module-02","slug":"/module-02/sensor-modeling","permalink":"/docs/module-02/sensor-modeling","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/06-sensor-modeling.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"06. Sensor Modeling: Cameras, LiDAR, IMU","sidebar_label":"06 Sensor Modeling"},"sidebar":"tutorialSidebar","previous":{"title":"05 Humanoid Locomotion","permalink":"/docs/module-02/humanoid-locomotion"},"next":{"title":"07 Noise & Latency","permalink":"/docs/module-02/noise-latency"}},{"id":"module-02/transfer-strategies","title":"11. Simulation-to-Real Transfer Strategies","description":"Deploy AI models from simulation to real humanoids with progressive testing, fine-tuning, and safety validation.","source":"@site/docs/module-02/11-transfer-strategies.md","sourceDirName":"module-02","slug":"/module-02/transfer-strategies","permalink":"/docs/module-02/transfer-strategies","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/11-transfer-strategies.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"11. Simulation-to-Real Transfer Strategies","sidebar_label":"11 Simulation-to-Real"},"sidebar":"tutorialSidebar","previous":{"title":"10 Domain Randomization","permalink":"/docs/module-02/domain-randomization"},"next":{"title":"Module 02 Summary","permalink":"/docs/module-02/module-summary"}},{"id":"module-02/unity-interaction","title":"08. Unity for Human–Robot Interaction","description":"Unity provides photorealistic rendering, interaction scenarios, and synthetic vision datasets for humanoid perception testing.","source":"@site/docs/module-02/08-unity-interaction.md","sourceDirName":"module-02","slug":"/module-02/unity-interaction","permalink":"/docs/module-02/unity-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-02/08-unity-interaction.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"08. Unity for Human–Robot Interaction","sidebar_label":"08 Unity Interaction"},"sidebar":"tutorialSidebar","previous":{"title":"07 Noise & Latency","permalink":"/docs/module-02/noise-latency"},"next":{"title":"09 Photorealism","permalink":"/docs/module-02/photorealism"}},{"id":"module-03/accelerated-computing","title":"01. Accelerated Computing in Physical AI","description":"GPUs accelerate physics, sensor rendering, and deep learning inference so humanoid robots can perceive and plan fast enough to stay upright and responsive.","source":"@site/docs/module-03/01-accelerated-computing.md","sourceDirName":"module-03","slug":"/module-03/accelerated-computing","permalink":"/docs/module-03/accelerated-computing","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/01-accelerated-computing.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"01. Accelerated Computing in Physical AI","sidebar_label":"01 Accelerated Computing"},"sidebar":"tutorialSidebar","previous":{"title":"Module 02 Summary","permalink":"/docs/module-02/module-summary"},"next":{"title":"02 Isaac Sim Architecture","permalink":"/docs/module-03/isaac-sim-architecture"}},{"id":"module-03/autonomy-stack","title":"11. End-to-End Autonomy Stack","description":"Isaac Sim enables end-to-end testing of sensors, SLAM, planning, and control so the AI brain can be validated before deployment.","source":"@site/docs/module-03/11-autonomy-stack.md","sourceDirName":"module-03","slug":"/module-03/autonomy-stack","permalink":"/docs/module-03/autonomy-stack","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/11-autonomy-stack.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"11. End-to-End Autonomy Stack","sidebar_label":"11 Autonomy Stack"},"sidebar":"tutorialSidebar","previous":{"title":"10 Path Planning","permalink":"/docs/module-03/path-planning"},"next":{"title":"Module 03 Summary","permalink":"/docs/module-03/module-summary"}},{"id":"module-03/gpu-physics-perception","title":"03. GPU-Accelerated Physics and Perception","description":"Physics and vision computations benefit from CUDA and tensor cores for dynamics, collision, and sensor simulation workloads.","source":"@site/docs/module-03/03-gpu-physics-perception.md","sourceDirName":"module-03","slug":"/module-03/gpu-physics-perception","permalink":"/docs/module-03/gpu-physics-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/03-gpu-physics-perception.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"03. GPU-Accelerated Physics and Perception","sidebar_label":"03 GPU Physics & Perception"},"sidebar":"tutorialSidebar","previous":{"title":"02 Isaac Sim Architecture","permalink":"/docs/module-03/isaac-sim-architecture"},"next":{"title":"04 Synthetic Data","permalink":"/docs/module-03/synthetic-data"}},{"id":"module-03/isaac-ros-hw","title":"07. Isaac ROS and Hardware Acceleration","description":"Isaac ROS exposes GPU-accelerated vision processing, mapping, and efficient ROS 2 communication with real hardware.","source":"@site/docs/module-03/07-isaac-ros-hw.md","sourceDirName":"module-03","slug":"/module-03/isaac-ros-hw","permalink":"/docs/module-03/isaac-ros-hw","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/07-isaac-ros-hw.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"07. Isaac ROS and Hardware Acceleration","sidebar_label":"07 Isaac ROS"},"sidebar":"tutorialSidebar","previous":{"title":"06 Visual SLAM","permalink":"/docs/module-03/visual-slam"},"next":{"title":"08 Localization & Mapping","permalink":"/docs/module-03/localization-mapping"}},{"id":"module-03/isaac-sim-architecture","title":"02. NVIDIA Isaac Sim Architecture","description":"Isaac Sim provides photorealistic rendering, GPU-accelerated physics, and tight ROS 2 integration so AI brains can be built and validated on high-fidelity simulations.","source":"@site/docs/module-03/02-isaac-sim-architecture.md","sourceDirName":"module-03","slug":"/module-03/isaac-sim-architecture","permalink":"/docs/module-03/isaac-sim-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/02-isaac-sim-architecture.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"02. NVIDIA Isaac Sim Architecture","sidebar_label":"02 Isaac Sim Architecture"},"sidebar":"tutorialSidebar","previous":{"title":"01 Accelerated Computing","permalink":"/docs/module-03/accelerated-computing"},"next":{"title":"03 GPU Physics & Perception","permalink":"/docs/module-03/gpu-physics-perception"}},{"id":"module-03/localization-mapping","title":"08. Localization and Mapping in Dynamic Environments","description":"Dynamic scenes require frequent map updates, sensor fusion, and adaptive planning to maintain accurate localization.","source":"@site/docs/module-03/08-localization-mapping.md","sourceDirName":"module-03","slug":"/module-03/localization-mapping","permalink":"/docs/module-03/localization-mapping","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/08-localization-mapping.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"08. Localization and Mapping in Dynamic Environments","sidebar_label":"08 Localization & Mapping"},"sidebar":"tutorialSidebar","previous":{"title":"07 Isaac ROS","permalink":"/docs/module-03/isaac-ros-hw"},"next":{"title":"09 Nav2 Navigation","permalink":"/docs/module-03/nav2"}},{"id":"module-03/module-summary","title":"Module 03 — Final Summary","description":"Isaac Sim provides a GPU-powered brain for humanoids. Synthetic data, Visual SLAM, Nav2, and physics-aware planning let AI-brain deployments stay safe, scalable, and ready for the real world.","source":"@site/docs/module-03/12-module-summary.md","sourceDirName":"module-03","slug":"/module-03/module-summary","permalink":"/docs/module-03/module-summary","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/12-module-summary.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"Module 03 — Final Summary","sidebar_label":"Module 03 Summary"},"sidebar":"tutorialSidebar","previous":{"title":"11 Autonomy Stack","permalink":"/docs/module-03/autonomy-stack"},"next":{"title":"01 VLA Overview","permalink":"/docs/module-04/vla-intro"}},{"id":"module-03/nav2","title":"09. Navigation with Nav2 for Humanoids","description":"Nav2 handles costmaps, path planning, obstacle avoidance, and footstep planning tailored to bipedal robots.","source":"@site/docs/module-03/09-nav2.md","sourceDirName":"module-03","slug":"/module-03/nav2","permalink":"/docs/module-03/nav2","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/09-nav2.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"09. Navigation with Nav2 for Humanoids","sidebar_label":"09 Nav2 Navigation"},"sidebar":"tutorialSidebar","previous":{"title":"08 Localization & Mapping","permalink":"/docs/module-03/localization-mapping"},"next":{"title":"10 Path Planning","permalink":"/docs/module-03/path-planning"}},{"id":"module-03/path-planning","title":"10. Path Planning Under Physical Constraints","description":"Humanoid robots require physics-aware planning to respect joint torque, balance, and collision avoidance.","source":"@site/docs/module-03/10-path-planning.md","sourceDirName":"module-03","slug":"/module-03/path-planning","permalink":"/docs/module-03/path-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/10-path-planning.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"10. Path Planning Under Physical Constraints","sidebar_label":"10 Path Planning"},"sidebar":"tutorialSidebar","previous":{"title":"09 Nav2 Navigation","permalink":"/docs/module-03/nav2"},"next":{"title":"11 Autonomy Stack","permalink":"/docs/module-03/autonomy-stack"}},{"id":"module-03/synthetic-data","title":"04. Synthetic Data Generation Pipelines","description":"Synthetic datasets deliver perfect annotations—diverse environments, labels, and scenarios—for robust perception training.","source":"@site/docs/module-03/04-synthetic-data.md","sourceDirName":"module-03","slug":"/module-03/synthetic-data","permalink":"/docs/module-03/synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/04-synthetic-data.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"04. Synthetic Data Generation Pipelines","sidebar_label":"04 Synthetic Data"},"sidebar":"tutorialSidebar","previous":{"title":"03 GPU Physics & Perception","permalink":"/docs/module-03/gpu-physics-perception"},"next":{"title":"05 Vision Models","permalink":"/docs/module-03/training-vision-models"}},{"id":"module-03/training-vision-models","title":"05. Training Vision Models for Robots","description":"Isaac Sim generates photorealistic images, depth maps, and masks that enable supervised learning for detection and segmentation.","source":"@site/docs/module-03/05-training-vision-models.md","sourceDirName":"module-03","slug":"/module-03/training-vision-models","permalink":"/docs/module-03/training-vision-models","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/05-training-vision-models.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"05. Training Vision Models for Robots","sidebar_label":"05 Vision Models"},"sidebar":"tutorialSidebar","previous":{"title":"04 Synthetic Data","permalink":"/docs/module-03/synthetic-data"},"next":{"title":"06 Visual SLAM","permalink":"/docs/module-03/visual-slam"}},{"id":"module-03/visual-slam","title":"06. Visual SLAM Fundamentals","description":"SLAM combines sensor data and odometry to estimate the robot's position, and Isaac ROS accelerates this with GPU support.","source":"@site/docs/module-03/06-visual-slam.md","sourceDirName":"module-03","slug":"/module-03/visual-slam","permalink":"/docs/module-03/visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-03/06-visual-slam.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"06. Visual SLAM Fundamentals","sidebar_label":"06 Visual SLAM"},"sidebar":"tutorialSidebar","previous":{"title":"05 Vision Models","permalink":"/docs/module-03/training-vision-models"},"next":{"title":"07 Isaac ROS","permalink":"/docs/module-03/isaac-ros-hw"}},{"id":"module-04/action-planning","title":"06. Action Planning from Language","description":"Translate language instructions into deterministic sub-goals and real-time controllers.","source":"@site/docs/module-04/06-action-planning.md","sourceDirName":"module-04","slug":"/module-04/action-planning","permalink":"/docs/module-04/action-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/06-action-planning.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"06. Action Planning from Language","sidebar_label":"06 Action Planning"},"sidebar":"tutorialSidebar","previous":{"title":"05 Speech & Voice","permalink":"/docs/module-04/speech-and-voice"},"next":{"title":"07 Dialog Management","permalink":"/docs/module-04/dialog-management"}},{"id":"module-04/dialog-management","title":"07. Dialog Management in Embodied AI","description":"Maintain multi-turn conversations while grounding context in sensor data and recent actions.","source":"@site/docs/module-04/07-dialog-management.md","sourceDirName":"module-04","slug":"/module-04/dialog-management","permalink":"/docs/module-04/dialog-management","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/07-dialog-management.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"07. Dialog Management in Embodied AI","sidebar_label":"07 Dialog Management"},"sidebar":"tutorialSidebar","previous":{"title":"06 Action Planning","permalink":"/docs/module-04/action-planning"},"next":{"title":"08 Feedback Loops","permalink":"/docs/module-04/feedback-loop"}},{"id":"module-04/feedback-loop","title":"08. Feedback Loops and Adaptation","description":"Continuous feedback from sensors and users lets VLA systems adapt behavior over time.","source":"@site/docs/module-04/08-feedback-loop.md","sourceDirName":"module-04","slug":"/module-04/feedback-loop","permalink":"/docs/module-04/feedback-loop","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/08-feedback-loop.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"08. Feedback Loops and Adaptation","sidebar_label":"08 Feedback Loops"},"sidebar":"tutorialSidebar","previous":{"title":"07 Dialog Management","permalink":"/docs/module-04/dialog-management"},"next":{"title":"09 Safety","permalink":"/docs/module-04/safety"}},{"id":"module-04/llms-in-vla","title":"02. Large Language Models for Robotics","description":"LLMs provide flexible instruction understanding, enabling natural queries, dialog management, and plan generation for robots.","source":"@site/docs/module-04/02-llms-in-vla.md","sourceDirName":"module-04","slug":"/module-04/llms-in-vla","permalink":"/docs/module-04/llms-in-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/02-llms-in-vla.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"02. Large Language Models for Robotics","sidebar_label":"02 LLMs"},"sidebar":"tutorialSidebar","previous":{"title":"01 VLA Overview","permalink":"/docs/module-04/vla-intro"},"next":{"title":"03 Vision Perception","permalink":"/docs/module-04/vision-perception"}},{"id":"module-04/module-summary","title":"Module 04 — Final Summary","description":"Vision-Language-Action weaves speech, vision, and language reasoning into safe, adaptive robotic behaviors. Multimodal fusion, planning, and feedback loops deliver embodied intelligence with natural interaction.","source":"@site/docs/module-04/11-module-summary.md","sourceDirName":"module-04","slug":"/module-04/module-summary","permalink":"/docs/module-04/module-summary","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/11-module-summary.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Module 04 — Final Summary","sidebar_label":"Module 04 Summary"},"sidebar":"tutorialSidebar","previous":{"title":"10 Personalization","permalink":"/docs/module-04/personalization"}},{"id":"module-04/multimodal-fusion","title":"04. Multimodal Fusion","description":"Fusing vision and language embeddings allows the robot to ground instructions into perceived objects.","source":"@site/docs/module-04/04-multimodal-fusion.md","sourceDirName":"module-04","slug":"/module-04/multimodal-fusion","permalink":"/docs/module-04/multimodal-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/04-multimodal-fusion.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"04. Multimodal Fusion","sidebar_label":"04 Multimodal Fusion"},"sidebar":"tutorialSidebar","previous":{"title":"03 Vision Perception","permalink":"/docs/module-04/vision-perception"},"next":{"title":"05 Speech & Voice","permalink":"/docs/module-04/speech-and-voice"}},{"id":"module-04/personalization","title":"10. Personalization and Context","description":"Tailor VLA responses based on user background, role, preferences, and the robot's current mission.","source":"@site/docs/module-04/10-personalization.md","sourceDirName":"module-04","slug":"/module-04/personalization","permalink":"/docs/module-04/personalization","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/10-personalization.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"10. Personalization and Context","sidebar_label":"10 Personalization"},"sidebar":"tutorialSidebar","previous":{"title":"09 Safety","permalink":"/docs/module-04/safety"},"next":{"title":"Module 04 Summary","permalink":"/docs/module-04/module-summary"}},{"id":"module-04/safety","title":"09. Safety and Alignment","description":"Safety reviews ensure instructions do not violate robot or human integrity, especially when chaining complex commands.","source":"@site/docs/module-04/09-safety.md","sourceDirName":"module-04","slug":"/module-04/safety","permalink":"/docs/module-04/safety","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/09-safety.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"09. Safety and Alignment","sidebar_label":"09 Safety"},"sidebar":"tutorialSidebar","previous":{"title":"08 Feedback Loops","permalink":"/docs/module-04/feedback-loop"},"next":{"title":"10 Personalization","permalink":"/docs/module-04/personalization"}},{"id":"module-04/speech-and-voice","title":"05. Speech and Voice Perception","description":"Speech recognition translates spoken commands into textual intent for the language-action pipeline.","source":"@site/docs/module-04/05-speech-and-voice.md","sourceDirName":"module-04","slug":"/module-04/speech-and-voice","permalink":"/docs/module-04/speech-and-voice","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/05-speech-and-voice.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"05. Speech and Voice Perception","sidebar_label":"05 Speech & Voice"},"sidebar":"tutorialSidebar","previous":{"title":"04 Multimodal Fusion","permalink":"/docs/module-04/multimodal-fusion"},"next":{"title":"06 Action Planning","permalink":"/docs/module-04/action-planning"}},{"id":"module-04/vision-perception","title":"03. Vision Perception for VLA","description":"Vision feeds the language-action loop with object awareness, scene understanding, and affordances.","source":"@site/docs/module-04/03-vision-perception.md","sourceDirName":"module-04","slug":"/module-04/vision-perception","permalink":"/docs/module-04/vision-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/03-vision-perception.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"03. Vision Perception for VLA","sidebar_label":"03 Vision Perception"},"sidebar":"tutorialSidebar","previous":{"title":"02 LLMs","permalink":"/docs/module-04/llms-in-vla"},"next":{"title":"04 Multimodal Fusion","permalink":"/docs/module-04/multimodal-fusion"}},{"id":"module-04/vla-intro","title":"01. VLA Overview","description":"Vision-Language-Action ties visual perception, natural language understanding, and actionable robot behavior to create intuitive humanoid interaction.","source":"@site/docs/module-04/01-vla-intro.md","sourceDirName":"module-04","slug":"/module-04/vla-intro","permalink":"/docs/module-04/vla-intro","draft":false,"unlisted":false,"editUrl":"https://github.com/Shafqatsarwar/first_hackathone/tree/main/packages/create-docusaurus/templates/shared/docs/module-04/01-vla-intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"01. VLA Overview","sidebar_label":"01 VLA Overview"},"sidebar":"tutorialSidebar","previous":{"title":"Module 03 Summary","permalink":"/docs/module-03/module-summary"},"next":{"title":"02 LLMs","permalink":"/docs/module-04/llms-in-vla"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro"},{"type":"doc","id":"book-overview"},{"type":"doc","id":"module-01/physical-ai-nervous-system","label":"01 Physical AI Nervous System"},{"type":"doc","id":"module-01/ros2-dds-philosophy","label":"02 ROS 2 & DDS Philosophy"},{"type":"doc","id":"module-01/nodes-cognitive-units","label":"03 Nodes as Cognitive Units"},{"type":"doc","id":"module-01/topics-services-actions","label":"04 Topics, Services, Actions"},{"type":"doc","id":"module-01/python-agents-rclpy","label":"06 Python Agents (rclpy)"},{"type":"doc","id":"module-01/llm-to-control","label":"07 LLMs & Control"},{"type":"doc","id":"module-01/parameters-runtime","label":"08 Parameters & Runtime"},{"type":"doc","id":"module-01/urdf-humanoid-body","label":"09 URDF & Body"},{"type":"doc","id":"module-01/actuators-feedback","label":"10 Actuators & Feedback"},{"type":"doc","id":"module-01/launch-composition","label":"11 Launch & Composition"},{"type":"doc","id":"module-02/digital-twins","label":"01 Digital Twins"},{"type":"doc","id":"module-02/physics-simulation","label":"02 Physics Simulation"},{"type":"doc","id":"module-02/gazebo-ros2","label":"03 Gazebo + ROS 2"},{"type":"doc","id":"module-02/gravity-collisions","label":"04 Gravity & Collisions"},{"type":"doc","id":"module-02/humanoid-locomotion","label":"05 Humanoid Locomotion"},{"type":"doc","id":"module-02/sensor-modeling","label":"06 Sensor Modeling"},{"type":"doc","id":"module-02/noise-latency","label":"07 Noise & Latency"},{"type":"doc","id":"module-02/unity-interaction","label":"08 Unity Interaction"},{"type":"doc","id":"module-02/photorealism","label":"09 Photorealism"},{"type":"doc","id":"module-02/domain-randomization","label":"10 Domain Randomization"},{"type":"doc","id":"module-02/transfer-strategies","label":"11 Simulation-to-Real"},{"type":"doc","id":"module-02/module-summary","label":"Module 02 Summary"},{"type":"doc","id":"module-03/accelerated-computing","label":"01 Accelerated Computing"},{"type":"doc","id":"module-03/isaac-sim-architecture","label":"02 Isaac Sim Architecture"},{"type":"doc","id":"module-03/gpu-physics-perception","label":"03 GPU Physics & Perception"},{"type":"doc","id":"module-03/synthetic-data","label":"04 Synthetic Data"},{"type":"doc","id":"module-03/training-vision-models","label":"05 Vision Models"},{"type":"doc","id":"module-03/visual-slam","label":"06 Visual SLAM"},{"type":"doc","id":"module-03/isaac-ros-hw","label":"07 Isaac ROS"},{"type":"doc","id":"module-03/localization-mapping","label":"08 Localization & Mapping"},{"type":"doc","id":"module-03/nav2","label":"09 Nav2 Navigation"},{"type":"doc","id":"module-03/path-planning","label":"10 Path Planning"},{"type":"doc","id":"module-03/autonomy-stack","label":"11 Autonomy Stack"},{"type":"doc","id":"module-03/module-summary","label":"Module 03 Summary"},{"type":"doc","id":"module-04/vla-intro","label":"01 VLA Overview"},{"type":"doc","id":"module-04/llms-in-vla","label":"02 LLMs"},{"type":"doc","id":"module-04/vision-perception","label":"03 Vision Perception"},{"type":"doc","id":"module-04/multimodal-fusion","label":"04 Multimodal Fusion"},{"type":"doc","id":"module-04/speech-and-voice","label":"05 Speech & Voice"},{"type":"doc","id":"module-04/action-planning","label":"06 Action Planning"},{"type":"doc","id":"module-04/dialog-management","label":"07 Dialog Management"},{"type":"doc","id":"module-04/feedback-loop","label":"08 Feedback Loops"},{"type":"doc","id":"module-04/safety","label":"09 Safety"},{"type":"doc","id":"module-04/personalization","label":"10 Personalization"},{"type":"doc","id":"module-04/module-summary","label":"Module 04 Summary"}]}}]},"chatbot":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/chatbot","tagsPath":"/chatbot/tags","isLast":true,"routePriority":-1,"sidebarFilePath":"D:\\Panaverse\\project\\first_hackathone\\docusaurus\\chatbot\\sidebars.js","contentPath":"D:\\Panaverse\\project\\first_hackathone\\docusaurus\\chatbot\\docs","docs":[{"id":"archived/README","title":"Archived docs","description":"The module and hardware folders were archived to keep the demo clean. If you need to restore any archived content, move files back into docusaurus/docs/ and update docusaurus/sidebars.js.","source":"@site/chatbot/docs/archived/README.md","sourceDirName":"archived","slug":"/archived/","permalink":"/chatbot/archived/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}},{"id":"book-chapter","title":"Demo Chapter — Project Requirements Summary","description":"This concise chapter maps the hackathon requirements into the demo deliverables and how the repository satisfies them.","source":"@site/chatbot/docs/book-chapter.md","sourceDirName":".","slug":"/book-chapter","permalink":"/chatbot/book-chapter","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"chatbotSidebar","previous":{"title":"Introduction to Physical AI & Humanoid Robotics","permalink":"/chatbot/"},"next":{"title":"Module 1: The Robotic Nervous System (ROS 2)","permalink":"/chatbot/module1_ros/content"}},{"id":"book-overview","title":"Book Overview — Physical AI & Humanoid Robotics","description":"This interactive textbook teaches how to design, simulate, and deploy humanoid robots that interact naturally with people and environments. The demo site includes:","source":"@site/chatbot/docs/book-overview.md","sourceDirName":".","slug":"/book-overview","permalink":"/chatbot/book-overview","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2}},{"id":"chapter1","title":"Module 1 — Introduction (Migrated)","description":"This is a migrated chapter from the textbook. Start with Module 1 to learn about ROS 2 and robotic middleware.","source":"@site/chatbot/docs/chapter1.md","sourceDirName":".","slug":"/chapter1","permalink":"/chatbot/chapter1","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Module 1 — Introduction (Migrated)"}},{"id":"chapter2","title":"Doc Two (Migrated)","description":"Content for doc2. Replace with real chapter content as needed.","source":"@site/chatbot/docs/chapter2.md","sourceDirName":".","slug":"/chapter2","permalink":"/chatbot/chapter2","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Doc Two (Migrated)"}},{"id":"chapter3","title":"Doc Three (Migrated)","description":"Content for doc3. Use this as a placeholder chapter.","source":"@site/chatbot/docs/chapter3.md","sourceDirName":".","slug":"/chapter3","permalink":"/chatbot/chapter3","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Doc Three (Migrated)"}},{"id":"intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Welcome to the Future of AI","source":"@site/chatbot/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/chatbot/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"slug":"/"},"sidebar":"chatbotSidebar","next":{"title":"Demo Chapter — Project Requirements Summary","permalink":"/chatbot/book-chapter"}},{"id":"module1_ros/content","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"Overview","source":"@site/chatbot/docs/module1_ros/content.md","sourceDirName":"module1_ros","slug":"/module1_ros/content","permalink":"/chatbot/module1_ros/content","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"chatbotSidebar","previous":{"title":"Demo Chapter — Project Requirements Summary","permalink":"/chatbot/book-chapter"},"next":{"title":"Module 2: The Digital Twin (Gazebo & Unity)","permalink":"/chatbot/module2_simulation/content"}},{"id":"module2_simulation/content","title":"Module 2: The Digital Twin (Gazebo & Unity)","description":"Overview","source":"@site/chatbot/docs/module2_simulation/content.md","sourceDirName":"module2_simulation","slug":"/module2_simulation/content","permalink":"/chatbot/module2_simulation/content","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"chatbotSidebar","previous":{"title":"Module 1: The Robotic Nervous System (ROS 2)","permalink":"/chatbot/module1_ros/content"},"next":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac™)","permalink":"/chatbot/module3_nvidia_isaac/content"}},{"id":"module3_nvidia_isaac/content","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac™)","description":"Overview","source":"@site/chatbot/docs/module3_nvidia_isaac/content.md","sourceDirName":"module3_nvidia_isaac","slug":"/module3_nvidia_isaac/content","permalink":"/chatbot/module3_nvidia_isaac/content","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"chatbotSidebar","previous":{"title":"Module 2: The Digital Twin (Gazebo & Unity)","permalink":"/chatbot/module2_simulation/content"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/chatbot/module4_vla/content"}},{"id":"module4_vla/content","title":"Module 4: Vision-Language-Action (VLA)","description":"Overview","source":"@site/chatbot/docs/module4_vla/content.md","sourceDirName":"module4_vla","slug":"/module4_vla/content","permalink":"/chatbot/module4_vla/content","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"chatbotSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac™)","permalink":"/chatbot/module3_nvidia_isaac/content"}}],"drafts":[],"sidebars":{"chatbotSidebar":[{"type":"doc","id":"intro"},{"type":"doc","id":"book-chapter"},{"type":"category","label":"Module Navigator","items":[{"type":"doc","id":"module1_ros/content"},{"type":"doc","id":"module2_simulation/content"},{"type":"doc","id":"module3_nvidia_isaac/content"},{"type":"doc","id":"module4_vla/content"}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.tsx"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-plugin-client-redirects":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}